{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example notebook for a single endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the supporting information to the manuscript entitled \"Conformal Prediction and \n",
    "Exchangeability in Toxicological In Vitro Datasets (title tbd)\". The notebook was developed by Andrea Morger in the \n",
    "In Silico Toxicology and Structural Biology Group of Prof. Dr. Andrea Volkamer at the Charité Universitätsmedizin \n",
    "Berlin, in collaboration with Fredrik Svensson, Ulf Norinder and Ola Spjuth. It was last updated in September 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "This notebooks demonstrates the main workflow to obtain the results for the manuscript on \"conformal prediction and exchangeability in in vitro toxicological datasets\" (todo: exchange with title of the manuscript) for an example endpoint.\n",
    "It can be used to train aggregated conformal predictors on the Tox21 endpoints. The predictions of Tox21 score can be compared in different experiments with and without updated calibration sets as well as with updating the complete training set. The notebook may be adapted to use the code for different datasets if a different endpoint is selected in `input cell 6.`\n",
    "\n",
    "For a general introduction on conformal prediction, exchangeability and calibration plots we refer to the manuscript.\n",
    "\n",
    "The notebook consists of three main parts.\n",
    "\n",
    "1. Preparation\n",
    "    - Used Python libraries are loaded\n",
    "    - Paths and parameters are defined\n",
    "2. Conformal prediction experiments\n",
    "    - Load data\n",
    "    - Prepare and perform experiments with different calibration sets\n",
    "    - Prepare and perform experiments with an updated training set\n",
    "3. Evaluate conformal predictions\n",
    "    - Calibration plots\n",
    "    - Calculate and plot rmsds\n",
    "    - Plot nonconformity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "1. [Preparation](#preparation) <br>\n",
    "    1.1. [Import libraries and modules](#import-libraries-and-modules)<br>\n",
    "    1.2. [Define paths and parameters](#define-paths-parameters)<br>\n",
    "2. [Conformal prediction experiments](#cp-experiments)<br>\n",
    "    2.1. [Load datasets](#load-data)<br>\n",
    "    2.2. [Prepare and perform experiments with different calibration sets](#exp-different-cal-sets)<br>\n",
    "    2.3. [Prepare and perform experiments with an updated training set](#exp-updated-train-set)<br>\n",
    "3. [Evaluate conformal predictions](#evaluate-cp)<br>\n",
    "    3.1. [Calibration plots](#cal-plots)<br>\n",
    "    3.2. [rmsd's](#rmsds)<br>\n",
    "    3.3. [nonconformity scores](#nc-scores)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation\n",
    "### 1.1. Import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from nonconformist.nc import NcFactory, MarginErrFunc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cptox21 import (\n",
    "    define_path, load_signatures_files, StratifiedRatioSampler,CrossValidationSampler,\n",
    "    KnownIndicesSampler, InductiveConformalPredictor,\n",
    "    AggregatedConformalPredictor, CPTox21AggregatedConformalPredictor, \n",
    "    CPTox21CrossValidator, CPTox21TrainUpdateCrossValidator, \n",
    "    calculate_rmsd_from_df, calculate_rmsd_pos_from_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Define paths and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_signatures_path = \"../data/data_signatures/\"\n",
    "data_statistics_path = \"../data/data_statistics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"NR_ER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of loops used in ACP\n",
    "n_folds_acp = 2  # 4  # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_state = 42 #  None  # To make the data reproducible, a random state can be set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conformal prediction experiments\n",
    "### 2.1. Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"train\", \"test\", \"score\"]  # Tox21 datasets\n",
    "train_path = define_path(endpoint=endpoint, data=datasets[0], signatures_path=data_signatures_path)\n",
    "test_path = define_path(endpoint=endpoint, data=datasets[1], signatures_path=data_signatures_path)\n",
    "score_path = define_path(endpoint=endpoint, data=datasets[2], signatures_path=data_signatures_path)\n",
    "\n",
    "X_train, y_train, X_test, y_test, X_score, y_score = load_signatures_files(train_path, test_path, score_path)\n",
    "\n",
    "# Uncomment below code for final run (shortens calculation time for test runs)\n",
    "X_train = X_train[:500]\n",
    "y_train = y_train[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare size of datasets\n",
    "To interpret and assess the results, it might be useful to know, how many data points we actually have per dataset, and how balanced they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>actives</th>\n",
       "      <th>inactives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>132.0</td>\n",
       "      <td>368.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>27.0</td>\n",
       "      <td>231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>score</td>\n",
       "      <td>49.0</td>\n",
       "      <td>441.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  actives  inactives\n",
       "0  train    132.0      368.0\n",
       "1   test     27.0      231.0\n",
       "2  score     49.0      441.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_size_dict = {}\n",
    "datasets_size_dict[\"name\"] = datasets\n",
    "datasets_size_dict[\"actives\"] = []\n",
    "datasets_size_dict[\"inactives\"] = []\n",
    "\n",
    "for dataset, labels in zip(datasets, [y_train, y_test, y_score]):\n",
    "    num_actives = np.sum(labels)\n",
    "    num_inactives = len(labels) - num_actives\n",
    "    datasets_size_dict[\"actives\"].append(num_actives)\n",
    "    datasets_size_dict[\"inactives\"].append(num_inactives)\n",
    "        \n",
    "\n",
    "pd.DataFrame.from_dict(datasets_size_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Prepare and perform experiments with different calibration sets\n",
    "#### Build ICP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', C=50, gamma=0.002, probability=True, random_state=set_random_state)\n",
    "error_function = MarginErrFunc()\n",
    "normaliser_model = None\n",
    "nc = NcFactory.create_nc(clf, err_func=error_function)\n",
    "icp = InductiveConformalPredictor(nc_function=nc, condition=(lambda instance: instance[1]))\n",
    "# Mondrian as (default) condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build ACP for main framework (excluding for train_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "acp = CPTox21AggregatedConformalPredictor(\n",
    "    predictor=icp, sampler=StratifiedRatioSampler(n_folds=n_folds_acp, random_state=set_random_state), aggregation_func=np.median\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define crossvalidator, which does all acp experiments except for train_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validator = CPTox21CrossValidator(\n",
    "    acp, cv_splitter=CrossValidationSampler(random_state=set_random_state), score_splitter=StratifiedRatioSampler(test_ratio=0.5, random_state=set_random_state)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crossvalidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len score 245 245\n",
      "len score 245 245\n",
      "len score 245 245\n",
      "len score 245 245\n"
     ]
    }
   ],
   "source": [
    "cross_validation_dfs = cross_validator.cross_validate(\n",
    "    steps=10,  # Number of steps (significance level) for evaluating conformal predictions\n",
    "    endpoint=endpoint,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_update=X_test,\n",
    "    y_update=y_test,\n",
    "    X_score=X_score,\n",
    "    y_score=y_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Prepare and perform experiments with an updated training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get splits from crossvalidator, so that exactly the same splits can be used for train_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, test_index = cross_validator.train_indices, cross_validator.test_indices\n",
    "\n",
    "known_indices_sampler = KnownIndicesSampler(known_train=train_index, known_test=test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define ACP for train_update\n",
    "Difference to CPTox21AggregatedConformalPredictor: As we do not further update the calibration set for this experiment, here, we can use a 'typical' ACP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_update_acp = AggregatedConformalPredictor(\n",
    "    predictor=icp, sampler=StratifiedRatioSampler(n_folds=n_folds_acp, random_state=set_random_state), aggregation_func=np.median\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define train_update crossvalidator using same splits as before and crossvalidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_update_cross_validator = CPTox21TrainUpdateCrossValidator(\n",
    "    train_update_acp, cv_splitter=known_indices_sampler\n",
    ")\n",
    "\n",
    "train_update_cross_validation_dfs = train_update_cross_validator.cross_validate(\n",
    "    steps=10,  # Number of steps (significance level) for evaluating conformal predictions\n",
    "    endpoint=endpoint,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_update=X_test,\n",
    "    y_update=y_test,\n",
    "    X_score=X_score,\n",
    "    y_score=y_score,\n",
    ")\n",
    "\n",
    "[df[[\"validity\", \"efficiency\", \"accuracy\", \"significance_level\"]].head() for df in train_update_cross_validation_dfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate conformal predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Calibration plots\n",
    "In a calibration plot, the observed error rate for a batch of predictions is plotted against the expected error rate (diagonal line). To provide some more information, we also plot the efficiency, defined as the ratio of single class predictions. The firm lines in the plot are the mean values of a five-fold cross-validation, the shaded areas represent the standard deviation.\n",
    "\n",
    "Calibration plots are useful for investigating the exchangeability between the datasets, and also to analyse the improvement strategies (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-validation\n",
    "As proof-of-concept, let's look at the calibration plot of the cross-validation. Given the random stratified splitting and sufficient data, the error rate should follow the diagonal line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots[\"cv\"] = cross_validator.calibration_plot( \n",
    "    averaged_evaluation_df=cross_validator.averaged_evaluation_df_cv,\n",
    "    endpoint=endpoint,\n",
    "    title_name=\"cross-validation with original calibration set\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict score and test set using the aggregated conformal predictor with the 'original' calibration set\n",
    "If the training set and the predicted data are drawn from the same distribution, and if they are available in sufficient amount, the error rates should follow the diagonal line. Any deviations suggest inexchangeability (or too few data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots[\"pred_score\"] = cross_validator.calibration_plot(\n",
    "    averaged_evaluation_df=cross_validator.averaged_evaluation_df_pred_score,\n",
    "    endpoint=endpoint, \n",
    "    title_name=\"predict score set (original calibration set)\"\n",
    ")\n",
    "\n",
    "plots[\"pred_test\"] = cross_validator.calibration_plot(\n",
    "    averaged_evaluation_df=cross_validator.averaged_evaluation_df_pred_test,\n",
    "    endpoint=endpoint, \n",
    "    title_name=\"predict test set (original calibration set)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update (increase) the training set with more recent data\n",
    "Probably the most intuitive way to improve the validity of the predictions will be to update the training set with more recent data. If \"old and new\" data are available, they can be combined to train a more up-to-date model. The following plots show the cross-validation of this model as well as the prediction of score data. \n",
    "\n",
    "For the prediction of the score data, we, usually (for most endpoints), do not see a big difference to the above calibration plot, since the number of recent compounds is almost negligible compared to the number of original training compounds. On the other hand, the more recent data would not be enough for training a model on them alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots[\"train_update_cv\"] = train_update_cross_validator.calibration_plot(\n",
    "    averaged_evaluation_df=train_update_cross_validator.averaged_evaluation_df_cv,\n",
    "    endpoint=endpoint, \n",
    "    title_name=\"cross-validation with updated training set\"\n",
    ")\n",
    "\n",
    "plots[\"train_update_pred_score\"] = train_update_cross_validator.calibration_plot(\n",
    "    averaged_evaluation_df=train_update_cross_validator.averaged_evaluation_df_pred_score,\n",
    "    endpoint=endpoint, \n",
    "    title_name=\"predict score set (updated training set)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update (exchange) the calibration set with a more recent dataset\n",
    "If exchangeability between the datasets cannot be assumed, a proposed strategy to improve the applicability of the model, is to update the calibration set with more recent data.\n",
    "Based on the chronogical release of the Tox21 datasets, we assume that Tox21test is more similar to Tox21 score than Tox21 train. Thus Tox21test is used to update the calibration set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots[\"cal_update\"] = cross_validator.calibration_plot(\n",
    "    averaged_evaluation_df=cross_validator.averaged_evaluation_df_cal_update,\n",
    "    endpoint=endpoint, \n",
    "    title_name=\"predict score set (updated calibration set)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update (exchange) the calibration set with part of the current prediction set\n",
    "With the above updating experiment, we made an assumption about the similarity of the datasets. This assumption was not true for all datasets. Thus, a more suitable experiment would be to update the calibration set with one part (50%) of the score set and to predict the other part of the score set. This gives us the certainty that the calibration and the prediction set are drawn from the same distribution. Note, that this might be more suitable as a proof of concept, on the other hand it does not represent a real-life scenario. Furthermore, we might see a larger standard deviation in the error rate, as we predict fewer data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots[\"cal_update2\"] = cross_validator.calibration_plot(\n",
    "    averaged_evaluation_df=cross_validator.averaged_evaluation_df_cal_update2,\n",
    "    endpoint=endpoint, \n",
    "    title_name=\"predict part of score set (updated calibration set \\n with (other) part of score set)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. rmsd's\n",
    "To have a value to compare the calibration plots (validity) over all experiments, we calculate the rmsd of the observed error rate to the expected error rate (for 10 significance levels). If we accept overconservative validity, we can also calculate rmsd_pos (todo: change term/name?), only considering the deviation where the observed error rate is higher than the expected error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect data (evaluation dfs) from the cross-validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dfs = {}\n",
    "evaluation_dfs[\"cv_original\"] = cross_validator.averaged_evaluation_df_cv\n",
    "evaluation_dfs[\"pred_score_original\"] = cross_validator.averaged_evaluation_df_pred_score\n",
    "evaluation_dfs[\"pred_test_original\"] = cross_validator.averaged_evaluation_df_pred_test\n",
    "evaluation_dfs[\"train_update\"] = train_update_cross_validator.averaged_evaluation_df_pred_score\n",
    "evaluation_dfs[\"pred_calupdate\"] = cross_validator.averaged_evaluation_df_cal_update\n",
    "evaluation_dfs[\"pred_calupdate2\"] = cross_validator.averaged_evaluation_df_cal_update2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate rmsd's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsds = {}\n",
    "rmsds_pos = {}\n",
    "for k, v in evaluation_dfs.items():\n",
    "\n",
    "    rmsd = calculate_rmsd_from_df(v)\n",
    "    rmsds[k] = rmsd\n",
    "    \n",
    "    rmsd_pos = calculate_rmsd_pos_from_df(v)\n",
    "    rmsds_pos[k] = rmsd_pos\n",
    "rmsds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot\n",
    "To visualise the rmsds over all strategies, a scatter plot can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rmsds(rmsds, strategies, endpoint=endpoint):\n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    \n",
    "    plt.scatter(strategies, [rmsds[s] for s in strategies], marker='_', s=500)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.title(f\"rmsds for different CP set-ups - {endpoint} endpoint\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = [\"cv_original\", \"pred_score_original\", \"pred_test_original\", \"train_update\", \"pred_calupdate\",\n",
    "             \"pred_calupdate2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rmsds(rmsds, strategies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Plot nonconformity scores\n",
    "For some endpoints, the above updating strategy (with more recent data/Tox21test) worked very nicely, for others it didn't. Plotting the nonconformity scores for the different datasets (this is done BEFORE calibration) might help to investigate the similarity of the datasets. Are Tox21train and Tox21score really very far from each other while the nonconformity scores Tox21test lie somewhere in between?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validator.plot_nonconformity_scores(cl=0, endpoint=endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validator.plot_nonconformity_scores(cl=1, endpoint=endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
